{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd983b1d-7748-47e3-95bd-f9ebb2c7dbab",
   "metadata": {},
   "source": [
    "# MultiSensor Dataset Preparation \n",
    "- Experiment data: March 2024. Aluminum, Laser-Wire DED\n",
    "- Aurthor: Chen Lequn\n",
    "\n",
    "### Notebook 1: Video Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84c15e44-7af9-4991-a519-58ce15c05229",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Required python libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.signal\n",
    "import os\n",
    "import math\n",
    "import sys\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import librosa\n",
    "import matplotlib.font_manager as font_manager\n",
    "\n",
    "#---------------opencv------------------------\n",
    "import cv2\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d8411ced-724d-429c-a5a6-925df20c63bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scaleogram as scg \n",
    "from glob import glob\n",
    "import scipy\n",
    "from scipy.signal import welch\n",
    "import wave                    # library handles the parsing of WAV file headers\n",
    "import pywt\n",
    "import soundfile as sf\n",
    "import glob\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d00f2471-373d-4feb-a277-d1d1ef7fc8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- plotly visualizatoin----------------------------------\n",
    "# from PIL import Image\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "from skimage import data\n",
    "\n",
    "from glob import glob\n",
    "import glob\n",
    "import scipy\n",
    "from scipy.signal import welch\n",
    "import wave                    # library handles the parsing of WAV file headers\n",
    "import pywt\n",
    "import soundfile as sf\n",
    "import matplotlib.font_manager as font_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5833c33f-e476-4cc6-a0a7-d40dfb25fad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Audio signal processing libraries\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import IPython.display as ipd\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "# import nussl\n",
    "import natsort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f895e1b8-6d69-47a6-ad36-ff4942e21a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee12e1ef-236d-4a31-9a0b-2addfe3913a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the figures, and dataset locations\n",
    "PROJECT_ROOT_DIR = \"../\"\n",
    "\n",
    "# Change to desirable location of the raw dataset\n",
    "Multimodal_dataset_PATH = \"F:\\Laser-Wire-DED-ThermalAudio-Dataset\"\n",
    "\n",
    "Dataset_path = os.path.join(Multimodal_dataset_PATH, f'25Hz')\n",
    "Video_path = os.path.join(Multimodal_dataset_PATH, 'Raw_Video')\n",
    "Audio_path = os.path.join(Multimodal_dataset_PATH, 'Audio')\n",
    "IMAGE_PATH = os.path.join(PROJECT_ROOT_DIR, \"result_images\", 'pre-processing')\n",
    "\n",
    "os.makedirs(IMAGE_PATH, exist_ok=True)\n",
    "os.makedirs(Dataset_path, exist_ok=True)\n",
    "os.makedirs(Audio_path, exist_ok=True)\n",
    "\n",
    "## function for automatically save the diagram/graph into the folder \n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGE_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "\n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "061cb9f9-bdf5-4014-a60c-7172cb660f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_visualization(sound, sr=44100, alpha = 1, fig_size = (10,2)):\n",
    "    fig, axs = plt.subplots(1, 1, tight_layout = True, figsize=fig_size) #constrained_layout=True,\n",
    "\n",
    "    librosa.display.waveplot(sound, sr=sr, alpha=alpha, label = 'original signal')\n",
    "    axs.set_xlabel('Time [sec]', fontsize = 14)\n",
    "    axs.set_ylabel('Amplitute', fontsize = 14)\n",
    "    # axs.set_ylim([-0.28, 0.28])\n",
    "    # axs.set_yticks([-0.25, 0, 0.25])\n",
    "\n",
    "    # fig.suptitle(\"Time-domain visualisation\", fontsize = 16)\n",
    "    \n",
    "def plot_spectrogram(spec, title=None, ylabel='Frequency bins', aspect='auto', xmax=None):\n",
    "    fig, axs = plt.subplots(1, 1)\n",
    "    axs.set_title(title or 'Spectrogram (db)')\n",
    "    axs.set_ylabel(ylabel)\n",
    "    axs.set_xlabel('frame')\n",
    "    im = axs.imshow(librosa.power_to_db(spec), origin='lower', aspect=aspect)\n",
    "    if xmax:\n",
    "        axs.set_xlim((0, xmax))\n",
    "        fig.colorbar(im, ax=axs)\n",
    "    # plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e73bbea3-5d86-4336-8b37-3f3bc867a678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.path.insert(0, 'utils')\n",
    "\n",
    "# import visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56593565-9c2b-4a72-a553-5ae9c8d56bab",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Segment AVI video file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224503e2-02f3-436e-9248-06029387682f",
   "metadata": {},
   "source": [
    "Methodologies:\n",
    "- __FFmpeg__:  extract the audio stream from the input MP4 file (__not recommended__) --> result in corrupted file\n",
    "- __moviepy__: extract audio from mp4 video stream. (`verified approach`)\n",
    "- __OpenCV__: extract the image frames from a video file (`verified approach`)\n",
    "- __iterates over each frame__ and uses FFmpeg/moviepy to extract the corresponding audio segment; OpenCV to extract the corresponding video segment\n",
    "- The audio and image segments are then saved to their respective output folders with the desired names."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f21530e-79cd-4efd-ab9d-e57ebf59cbe0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Single file segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70152f39-6892-496d-a971-d01a441373f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_index = 25\n",
    "\n",
    "# ----------------Define input video file path---------------\n",
    "raw_video_file = os.path.join(Video_path,  f'Exp_{sample_index}.avi')\n",
    "\n",
    "# Output folders\n",
    "raw_audio_output_folder = os.path.join(Multimodal_dataset_PATH,  '25Hz', str(sample_index), 'raw_audio')\n",
    "image_output_folder = os.path.join(Multimodal_dataset_PATH, '25Hz',str(sample_index), 'images')\n",
    "\n",
    "# Create output folders if they don't exist\n",
    "if not os.path.exists(image_output_folder):\n",
    "    os.makedirs(image_output_folder)\n",
    "if not os.path.exists(raw_audio_output_folder):\n",
    "    os.makedirs(raw_audio_output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06c782f1-76bc-4e94-9d3e-8c6d16ce3421",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in F:\\Laser-Wire-DED-ThermalAudio-Dataset\\Audio/25_raw.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "### ------------------Extract audio (wav) from video (MP4/avi) ------------------\n",
    "### ---------For single video file -----------\n",
    "# Extract the audio\n",
    "audio = VideoFileClip(raw_video_file).audio\n",
    "# Save the audio to a file\n",
    "audio.write_audiofile(f'{Audio_path}/{sample_index}_raw.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa0ecd4-7891-420d-a7a9-c8bb8b859a78",
   "metadata": {},
   "source": [
    "### Testing for single a single video file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94612fea-0c03-47b9-974c-973106595da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling rate: 44100\n",
      "the total number of samples segmented from this video is: 1764\n",
      "Each sample length in second is: 0.04\n",
      "total duration of audio signal: 29.29\n",
      "total number of images: 731.25\n"
     ]
    }
   ],
   "source": [
    "### -----------------For single video file ---------------\n",
    "# Load the audio and calculate the segment length in samples\n",
    "y_raw, sr = librosa.load(f'{Audio_path}/{sample_index}_raw.wav', sr=None)\n",
    "\n",
    "segment_duration = 0.04  # 0.04 seconds. 25 Hz\n",
    "segment_length = int(segment_duration * sr)\n",
    "print (f\"sampling rate: {sr}\")\n",
    "print (\"the total number of samples segmented from this video is: \" + str(segment_length))\n",
    "print (\"Each sample length in second is: \" + str(segment_length/44100))\n",
    "print (\"total duration of audio signal: \" + str(len(y_raw)/44100))\n",
    "print (\"total number of images: \" + str((len(y_raw) - segment_length)/segment_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1218f58b-2a0e-4b5b-a4fa-7104dbf2a6fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1289925"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_raw) - segment_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3e7b1c2-c718-4755-87fc-6217a39ef613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1764"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bc7df9-9f19-405d-a8a1-9e50ccd257d8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Method: direct segmentation\n",
    "- No past data was used\n",
    "- Only the current time frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97649033-72ba-4a4f-bc62-82a897681d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4abf05fe95954f5ba7ee615f1bd04403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/731 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to read frame for counter: 695\n",
      "Failed to read frame for counter: 696\n",
      "Failed to read frame for counter: 697\n",
      "Failed to read frame for counter: 704\n",
      "Failed to read frame for counter: 705\n",
      "Failed to read frame for counter: 706\n",
      "Failed to read frame for counter: 707\n",
      "Failed to read frame for counter: 708\n",
      "Failed to read frame for counter: 709\n",
      "Failed to read frame for counter: 713\n",
      "Failed to read frame for counter: 714\n",
      "Failed to read frame for counter: 715\n",
      "Failed to read frame for counter: 716\n",
      "Failed to read frame for counter: 717\n",
      "Failed to read frame for counter: 718\n",
      "Failed to read frame for counter: 719\n",
      "Failed to read frame for counter: 720\n",
      "Failed to read frame for counter: 721\n",
      "Failed to read frame for counter: 722\n",
      "Failed to read frame for counter: 723\n",
      "Failed to read frame for counter: 724\n",
      "Failed to read frame for counter: 725\n",
      "Failed to read frame for counter: 726\n",
      "Failed to read frame for counter: 727\n",
      "Failed to read frame for counter: 728\n",
      "Failed to read frame for counter: 729\n",
      "Failed to read frame for counter: 730\n",
      "Failed to read frame for counter: 731\n",
      "Failed to read frame for counter: 732\n"
     ]
    }
   ],
   "source": [
    "# Initialize a counter for audio and image file names\n",
    "counter = 1\n",
    "segment_length_samples = int(0.04 * sr)  # 40ms corresponds to 0.04 seconds, 25 Hz\n",
    "raw_seg = []\n",
    "\n",
    "\n",
    "# Initialize video capture once before the loop\n",
    "cap = cv2.VideoCapture(raw_video_file)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# Calculate total iterations for tqdm\n",
    "total_iterations = (len(y_raw) - segment_length_samples) // segment_length_samples\n",
    "\n",
    "# Iterate over the audio and save each segment\n",
    "for i in tqdm(range(0, len(y_raw) - segment_length_samples, segment_length_samples), total=total_iterations):\n",
    "    # print (i)\n",
    "    # Extract the audio segment\n",
    "    segment_raw = y_raw[i:i+segment_length]\n",
    "    raw_seg.append(segment_raw)\n",
    "  \n",
    "    sf.write(f'{raw_audio_output_folder}/sample_{sample_index}_{counter}.wav', segment_raw, samplerate = sr)\n",
    "   \n",
    "    # Use OpenCV to extract the video segment\n",
    "    frame_index = round(i / sr * fps)  # Rounding might give a closer frame\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index)\n",
    "    ret, frame = cap.read()\n",
    "    if ret:  # Check if frame read successfully\n",
    "        cv2.imwrite(f'{image_output_folder}/sample_{sample_index}_{counter}.jpg', frame)\n",
    "    else:\n",
    "        print(f\"Failed to read frame for counter: {counter}\")\n",
    "    \n",
    "    # Increment the counter\n",
    "    counter += 1\n",
    "\n",
    "cap.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
